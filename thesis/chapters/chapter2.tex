\section{Review of PubChem Assay 1851}
  

PubChem BioAssay 1851 contains data for inhibition of five major CYP isoforms (1A2, 2C9, 2C19, 2D6 and 3A4) by 17,143 chemical compounds. \cite{Veith2009} The tested compounds were all drugs or drug-like compounds. A look at the chemical space occupied by these compounds reveals that the majority of them had molecular weight below 500 daltons and logP below 5. \cite{Lapins2013}

PubChem Assay 1851 used low low-luminescent substrates, which are converted to more-luminescent metabolites as the signal for this assay. The progression of a reaction was measured by an increase in light intensity during CYP metabolism of the substrate. Inhibitors of a particular CYP isozyme reduced the rate of metabolism of the substrate and thus results in a decreased luminescent signal. \cite{Zlokarnik2005}

The most recent technology developed for CYP inhibition is based on substrates that release luciferin as the metabolite. This is a coupled assay system in which addition of luciferase and ATP converts the freed luciferin to des-carboxyluciferin along with light emission. The format is similar to fluoresence methods, but particularly well-suited to hight throughput applications because and all that is required are addition-only manipulations and luminescence plate readers. \cite{Zlokarnik2005} The assay obtained lumiescence readings at a range of compound concentrations and then determined activity parameters using the Hill equation.

In the PubChem Assay 1851, compounds are classified as active or inactive inhibitors for each CYP with an activity cutoff set to AC50 = 10uM (AC50, “activity concentration 50”, refers to the concentration that is required to elicit half-maximal effect). However, in cases where the dose-response curve for a compound showed poor fit or the inhibition efficacy was below 60\%, the assay results were regarded as inconclusive. \cite{Veith2009}

Compounds were characterized by their Activity Score and regarded as inhibitors if their activity score ranged between 40 and 100. An Acitivity Score is assigned based on AC50 value, which was combined with a confidence measure. Combining measures for completeness of a dose-response curve and efficacy of inhibition, resulted in the Activity Score, where a larger value indicates higher inhibitory activity and/or higher confidence in inhibitory assay result. Compounds with an activity score equal to zero are considered as non-inhibitors while compounds with activity scores above 0 and up to 40 are considered inconclusive. \cite{Lapins2013}

The data from Pubchem Assay 1851 is available for download from the NIH through the PubChem website. The interface changes from time to time, but I downloaded two files which comprised the entire dataset for the experiment. The first file, a structures file, contained the structural information encoded in the simplified molecular-input line-entry system (SMILES) format for each tested compound with corresponding Structure ID (SID) and Compound ID (CID) as assigned by NCBI. Another file, also organized by SID and CID, contained all of the luminescent responses from the high-throughput screen and the fitted parameters which are then summarized by the Activity Score. 
 

\section{Data Set Preparation and Molecular Descriptor Generation }

Both data files from Bioassy 1851 were downloaded as comma separated value (.csv) files and merged together based on the SID column using functions from Python's pandas library, which is made for handling, manipulating and reshaping structured data.

The preliminary steps of data preprocessing typically require data cleaning because raw data often contain anomalies, errors, or inconsistencies such as missing data, incomplete data, and invalid character values which may cause trouble in data analysis if left untreated. It is more complicated when data are collated from many formats that require harmonization and redundancy elimination. \cite{Nantasenamat2009} In this case, minimal preprocessing was required due to the polished nature of the source files.


\begin{figure}[h,t]
  \centering
   \includegraphics[width=1\textwidth]{../img/Dataset_prep.png}
  \caption{Data Download and Preparation}
\end{figure}


\subsubsection{Molecular Descriptor Generation}
First the merged file was loaded into a database in the Molecular Operating Environment. MOE functionality was used to obtain the washed configuration of compounds by removing the salts and finding an energy-minimized conformation. MOE was then used to calculate descriptors based on the molecular structures. The entire suite of MOE 2-D descriptors was selected for descriptor generation, resulting in 186 additional columns of nominal, ordinal and continuous vales appended to the database. The resulting master file was saved in .csv format.

The dataset was then split by isozyme into 5 separate files that each contained only the SID, the Activity Score, and the 186 MOE 2-D descriptors using a script in the Python programming language.

For each isozyme, the number of active inhibitors was far outnumbered by the number of inactive inhibitors. The data file for each isozyme was subjected to a Python script that separated the inactives from actives. Then the order of the inactives was randomly shuffled and the column trimmed to the length of the activity column, thereby balancing the number of inactives and actives as required by some of the later statistical analyses. 

\begin{figure}[H]
  \centering
   \includegraphics[width=.6\textwidth]{../img/Isozyme_Data_Prep.png}
  \caption{Isozyme Data Set Preparation}
\end{figure}

Next the script randomly shuffled the balanced datasets and split them into a training set and a test set. The training set held 80\% of the original values and the test set held 20\% of the original values. The split was not based on activity score. The ratio of active/inactive in each split was inspected to check if they were still acceptably balanced.

For each use of randomness in computation, a seed number was set for the random number generator to ensure reproducible results.  

The balanced and split datasets are saved to Figshare.com for permanent, free and open access. (http://dx.doi.org/10.6084/m9.figshare.1181846) and \\
(http://dx.doi.org/10.6084/m9.figshare.1066108) All subsequent analyses use these same splits for comparability.

\section{Data Preprocessing}

Typically data sets often contain redundant or noisy variables which make it more difficult for learning algorithms to discern meaningful patterns from the input. Such multicollinearity of the variables can either be used or treated if necessary to reduce computational resources required for model construction. \cite{Nantasenamat2009}

There existed a great deal of variability in the range and distribution of each variable in the data set. This can pose problems for algorithms using distance measurements in the learning step. These situations are handled by applying statistical techniques such as min-max normalization or z-score standardiation. 

In min-max normalization, the minimum and maximum value of each variable is adjusted to a uniform range between 0 and 1 according to the following equation: $ X_{norm} = \frac{X - X_{min}}{X_{max} - X_{min}}  $ In z-score standardization, essentially the variable of interest is subjected to statistical operation to achieve mean center and unit variance according to the following formula: $ z = \frac{x - \mu}{\sigma}  $ \cite{Nantasenamat2009}

In situations where the data does not have a Gaussian (normal) distribution, simple mathematical functions can be applied to achieve normality or symmetry in the data distribution. A commonly used approach is to apply logarithmic transformation on the the variable of interest in order to achieve distribution approaching normality. This is typically performed on dependent variables such as the modeled biological/chemical properties of interest whereby IC50 may be transformed to logIC50 or -logIC50. Practically, such mathematical operations are applied to each individual value of a given variable of interest. \cite{Nantasenamat2009}

\section{Modeling - Mapping Descriptors to Activity Data}

At this point a complete dataset has been assembled for each isozyme that is balanced for the number of active and inactive inhibitors. That dataset was then further subdivided into a traing set and a test set for later validation of results. The preceding sections described the different methods of feature selection and normalization and the following sections outline classification algorithms and validation procedures used in this study.

\begin{figure}[H]
  \centering
   \includegraphics[width=1\textwidth]{../img/Model_validation.png}
  \caption{Supervised Classification Model Training and Validation}
\end{figure}

\subsection{Binary QSAR in the Molecular Operating Environment}

The Binary QSAR approach described by Labute \cite{Labute1999} and as implemented in the 2011 version of MOE, takes a Bayesian and probabilistic approach to classification of activity after reducing all the descriptors to principal components. The initial principal component analysis (PCA) of the descriptor measurements reduce the dimensionality of the dataset to a smaller set of dimensions where the new axes represent the directions of greatest variance/spread of the data. The new axes are ranked according to the amount of variance each explains. Therefore the first principal component accounts for the most variance, while the second accounts for less variance and so on down the line. Models are subsequently built with the original data projected onto any or all of these principal components.

Initially, Partial Least Squares regression, a quantitative method available in the Molecuar Operating Environment that also begins with data reduction by PCA, was tried in Dr. Zheng's lab but has shown poor predictive accuracy with this dataset as demonstrated by previous attempts in practice.

To carry out Binary QSAR, the training data was loaded into MOE and then a menu driven interface was used to initiate the Binary QSAR method. A threshold Acitivity Score value of 39 was selected; meaning all activity values 40 and above were considered active and all values 39 and below were considered inactive. The smoothing parameter was left at the default value of 0.25. MOE automatically performs principal component analysis on high-dimensional datasets before Binary QSAR and there was no way to use the full suite of molecular descriptors untransformed.

For each isozyme, a number of models were built, each using a different number of principal components during model building.  Each principal component is orthogonal and uncorrelated to the rest, and so each one captures a portion of the total variance inherent in the dataset. The assumption is that inclusion of more principal components leads to more of the variance being accounted for in a classification decision. However, since each principal component is a linear combination of all the variables, the benefits of dimensionality reduction comes at the cost of interpretability. One result of this study will be to determine how many principal components are necessary to achieve good prediction results with the most dimensionality reduction i.e. the fewest number of principal components.  For exploratory purposes, models with 2, 5, 10, 15, 20, 30, and 44 principal components were constructed. 

MOE models were written to .fit files and the model report saved as a .txt file.

\subsubsection{Modeling in Python}
The Python programming language is a dynamically-typed, object-oriented, interpreted language. Its primary strength lies in the ease with which it allows a programmer to rapidly prototype a project, coupled with a powerful and mature set of standard libraries, like the scikit-learn package for machine learning. Python has a very shallow learning curve and easily accessible, online learning materials. The following machine learning classifiers used in this study were implemented with Python's scikit-learn package.

\subsection{$\kappa$-Nearest Neighbor ($\kappa$NN)}
The $\kappa$NN algorithm predicts the class of a test set object based on the class membership of its $\kappa$ most similar training set objects. \cite{Lapins2013} $\kappa$-nearest neighbors algorithms find the $\kappa$-points that are closest to a point in question based on their attributes using a certain distance measure (e.g., Euclidean distance), and then assign identity based on majority vote of those neighbor identities.

\subsection{Random Forest (RF)}
Random Forest is a classifier that consists of multiple decision trees. To borrow the language of graph theory, decision trees are made of nodes and branches. At each node, the dataset is split based on the value of some attribute that is selected so that the instances of different classes are predominately moved to different branches. Classification starts at the root node and is performed by passing the instances along the tree to leaf nodes. To introduce diversity between the trees of a random forest, a small subset of all features is randomly selected to take decisions at each node of each tree. The classification decision is performed by considering results of all trees in a majority vote. \cite{Lapins2013}

\subsection{Support Vector Machines (SVM)}
SVM is a machine learning technique for classification that tries to find the hyperplane that gives the greatest margin of separation between classes. SVMs apply a kernel function to each instance which projects the data into a higher-dimensional feature space before finding the hyperplane. \cite{Lapins2013} The margin is defined as the minimum distance from data points to the hyperplane. The data points with the smallest margin are called support vectors and used to establish the position of the final SVM classifier.

\section{Model Evaluation and Validation}

The building blocks of a successful QSAR model are the accuracy of the input data, selection of appropriate descriptors and statistical tools, and most importantly validation of the developed model. Validation is the process where reliability and relevance of a procedure are established for a predetermined purpose. For QSAR models validation should target robustness, predicition performances and applicability domain (AD) of the models. \cite{Lapins2013}

Statistical evaluation in QSAR modeling is essential to validate the model as well as to evaluate its predictive performance. The predictive performance of a data set can be assessed by dividing it into a training set and a testing set. The training set is used for constructing a model and then the predictive performance of that model is evaluated on the testing set. Internal performance is typically assessed from the predictive performance within a training set, while external performance of a classifier can be assessed from the predictive performance against an independent test set that has never been seen by the training model. 

Two statistical metrics were used for evaluations in this study -- the overall prediction Accuracy and the True Positive/Negative Rate. A commonly used approach for model assesment in the exploratory phase is known as N-fold cross-validation. This is when a data set is partitioned into N number of folds and one fold is left out of model construction. For example, in a 5-fold cross-validation 1 partition is left out as an internal testing set while the remaining 4 folds are used as the training set for model construction. Validation is then performed with the fold that was left out. In situations where the number of samples is limited, leave-one-out cross-validation is the preferred approach. In that case, the number of folds is equal to the number of samples present in the data set. \cite{Nantasenamat2009} Using these methods with the original training set allows the original test set to be preserved for final validation.

In this study, 5-fold cross validation was used in conjuction with model building on the training set for all of the models built in Python. Once model building and selection was complete, the test set was used for external validation. Therefore we assessed the predictive ability of the models by performing cross-validation and external predictions.

\subsubsection{Confusion Matrix}

A confusion matrix (or error matrix or table of confusion) is a representational summary of the performance of a classifier. All data points used in model building are apportioned along one axis according to their actual class value, and along another axis according to the value predicted by that model. For binary classification this results in a square matrix with two 2 columns and 2 rows.

\begin{tabular}{c @{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}
  \multirow{10}{*}{\parbox{1.1cm}{\bfseries\raggedleft Actual\\ Class}} & 
    & \multicolumn{2}{c}{\bfseries Prediction Class} & \\
  & & \bfseries p & \bfseries n &                     \\
  & p$'$ & \MyBox{True}{Positive} & \MyBox{False}{Negative} & \\[2.4em]
  & n$'$ & \MyBox{False}{Positive} & \MyBox{True}{Negative} & \\
  &  &  &  &
\end{tabular}

In this study, results that are both actual inhibitors according to assay data and predicted inhibitors according to the model results are deemed True Positives (TP). Similarly, actual noninhibitors that are predicted to be so are called True Negatives (TN). In the case of actual inhibitors predicted to be noninhibitors, these are labelled False Negatives (FN) and are the result of Type I errors. Noninhibitors classified as inhibitors by the model are refered to as False Positives (FP) and are the result of Type II errors.

\subsubsection{Accuracy}

The accuracy metric provides general information about how many compounds are misclassified. Accuracy is simply the percentage of correctly classified instances over the total number of instances and is calculated as
$$ ACC =\frac{(TP + TN)}{(TP + FP + TN + FN)} $$
where TP is the number of true positives, TN is the number of true negatives, FP is the number of false positives or over-predictions, and FN is the number of false negatives or missed predictions.

Accuracy is not an optimal measure of model performance if the data set is unbalanced (i.e. sizes of the classes are unequal) or if certain types of errors are to be considered more serious than others (e.g. false negatives compared to false positives). \cite{Lapins2013} For this reason, all datasets were rebalanced to contain roughly equal numbers of active and inactive inhibitors.

As a futher interrogation of results, the true positive rate and the true negative rate are also calculated for all models. The true positive rate is the fraction of true positives out of all actual inhibitors and it is calculated as follows 
$$ True Positive Rate = \frac{TP}{(TP + FN)} $$
The true positive rate is also refered to as Sensitivity and in MOE software it is called Accuracy on Active.

The true negative rate is the fraction of true negatives over the number of actual noninhibitors and is calculated as :
$$ True Negative Rate =\frac{ TN }{(TN + FP)} $$
The true negative rate is also know as the Specificity of a classification, and in MOE Binary QSAR it is called Accuray on Inactives.

\subsubsection{Binary QSAR after PLS of descriptors}
The test sets were loaded and the washed structures were appended to the .csv files. All models were evaluated using the menu driven workflow in MOE and the classification probabilities were appended to the database file and saved as a .csv.  

The resulting file was loaded into a Microsoft Excel spreadsheet and the predictions classified as actives or inactives. Predicted probabilities of $\geq$ 0.5 were evaluated as active inhibitor predictions and predicted probabilities of < 0.5 were deemed inactive inhibitors. Confusion matrices were then tabulated within the spreadsheet for predicted vs actual actives and inactives. And from these confusion matrices, classification accuracy measures were calculated - total accuracy, TPR, and TNR. These results were saved and the results reported below.

\subsubsection{$\kappa$-NN Classification Utilizing the Full Set of 2-D Descriptors}
The full models follow a similar procedure to the previous workflow, except they omit data reduction by principal component analysis and use the full descriptor set to construct and evaluate models. 

For each isozyme separately, training data is loaded into a dataframe. The Activity Score is assigned the role of response variable and all 186 descriptors are identified as predictor variables. Predictor variables are scaled and normed to mean $0$ and standard deviation of $1$. kNN.fit method is called from the scikit-learn library to train a model, which then reports the confusion matrix and an accuracy score for the training model by five-fold cross-validation. TPR and TNR are calculated from the confusion matrix.

\subsubsection{Random Forest Classification Utilizing the Full Set of 2-D Descriptors}
For each isozyme separately, training data is loaded into a dataframe. The Activity Score is assigned the role of response variable and all 186 descriptors are identified as predictor variables. Predictor variables are scaled and normed to mean $0$ and standard deviation of $1$. RF.fit method is called from the scikit-learn library to train a model, which then reports the confusion matrix and an accuracy score for the training model by five-fold cross-validation. TPR and TNR are calculated from the confusion matrix.

\subsubsection{Support Vector Machine Classification Utilizing the Full Set of 2-D Descriptors}
For each isozyme separately, training data is loaded into a dataframe. The Activity Score is assigned the role of response variable and all 186  descriptors are identified as predictor variables. Predictor variables are scaled and normed to mean $0$ and standard deviation of $1$. SVM.fit method is called from the scikit-learn library to train a model, which then reports a confusion matrix and an accuracy score for the training model by five-fold cross-validation. TPR and TNR are calculated from the confusion matrix.

A script was written for each isozyme that performed these three fit methods in series. Code and results are documented in IPython notebooks that are currently hosted on github.com (https://github.com/5x5x5x5/CYP/) and freely accessible and downloadable. Presented in this way, they are more easily verifyable and extendable.  Because of the simple and consistent design of the scikit-learn library, experimenting with other classification algorithms simply requires adding new method.fit calls to the model building loop.


\begin{figure}[!htbp]
  \centering
  \includegraphics[width=1\textwidth]{../img/supervised_learning_flowchart.png}
  \caption{Supervised Learning Workflow for Clasification}
\end{figure}
