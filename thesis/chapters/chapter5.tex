
\section{Other Attempts at Modeling Assay 1851}
Pubchem Bioassy AID1851 has been the basis for several attempts to advance \textit{in silico} screening development. Several research groups have developed large-scale single target models for individual isoforms. \cite{Vasanthanathan2009} \\\cite{Sridhar2012} \cite{Sun2012} \cite{Novotarskyi2010} Another group pursued all five isozymes but used a proteochemometric method that also takes into account each isozyme's protein sequence. \cite{Lapins2013} And finally Cheng et al. created single target QSAR models for all five CYP isoforms and these are the most comparable to my efforts. \cite{Cheng2011} These models all showed good predictive performances according to our standards of a 0.6 accuracy threshold.

In Cheng's study, inhibition prediction models were developed for the same five major CYP isoforms, namely 1A2, 2C9, 2C19, 2D6 and 3A4 as described previously. However their approach used a combined classifier algorithm on a large data set containing more than 24,700 unique compounds. These compounds also came from PubChem Bioassay 1851 and included the 17,143 used in this study, plus additional compounds from PubChem ioassay 410. His group used an ensemble of different independent machine learning classifiers including support vector machine, C 4.5 decision tree, $\kappa$-nearest neighbor, and naive Bayes. The final ensemble was brought together using a back-propagation artificial neural network (BP-ANN). Those models were also internally validated by 5-fold cross-validation and then evaluated with a test set composed of about 9000 diverse unique compounds previously unseen by the classifier. The range of the validation measures from the test set results was from 0.764 to 0.886. These results are similar to the findings presented in this thesis. Cheng et al. claim these classification models are applicable for virtual screening of the five major CYP isoforms inhibitors or can be used as simple filters of potential chemicals in drug discovery. \cite{Cheng2011}


\section{Sources of Error}
This assay required recombinant sources of CYP enzymes because the probes are not sufficiently P450 isoform-selective to be used with those derived from human liver microsomes. Another potential source for false positives in these assays can be compounds that interfere with light generation directly, such as compounds that interfere with luciferase enzymatic activity. \cite{Zlokarnik2005}

The results of the large-scale screening of Pubchem AID1851 against five CYP isoforms identified that the majority of compounds in a typical chemical library cross-inhibited several isoforms, while only a small fraction of the compounds did not inhibit any of the isoforms. \cite{Veith2009}

The phenomenon of uniformly lower accracy on the inactives versus the actives might be explained by the composition of the inactive class. Both inactive inhibitors as well as compounds of uncertain activity were both included under the label 'inactive' for binary classification purposes. The chosen representation of class inclusion may have less potential for successful discrimination than models with more possible classes. To test assumptions about class labels, the number of classes could be increased or the Activity Score threshold could be altered in future models.


\section{Machine Learning for Pharmaceutical Sciences}
\begin{quote}
Developing successful machine learning applications still requires a substantial amount of “black art” that is hard to find in textbooks. \cite{Domingos2012}
\end{quote}

Unlike in this study, raw data is often not in a form that is amenable to learning, but you can construct features from it that are. Easily the most important factor in a good model are the features used. If there are many independent features that correlate well with the class, learning is easy. However if the class is a very complex function of the features, the time required to train a good model may take to long or be impossible. \cite{Domingos2012} Most of the time and effort in a machine learning project typically goes into feature engineering, and it is one area where domain expertise and experience with chemical library design can mean the difference between success and failure.

Also, machine learning is not a one-shot process of building a data set and running a learner, but rather an iterative process of running the learner, analyzing results, modifying the data and/or learner, and repeating. Learning is often the quickest part of this, but only because of all the  effort that has gone into sharing and codifying prior knowledge. Feature engineering is considered more difficult because it is domain-specific, while learners can by and large be general-purpose. \cite{Domingos2012}

There is ultimately no replacement for domain expertise in feature engineering when the other option is to run a learner with a very large number of features to find out which ones are useful in combination. The later approach may be too time-consuming, or cause overfitting. \cite{Eklund2014}

As a rule, it pays to try the simplest learners first (e.g. naive Bayes before logistic regression, $\kappa$-nearest neighbor before support vector machines). More sophisticated learners may be seductive, but they are usually harder to use, because there are more tuning parameters and their internals are more opaque. \cite{Domingos2012}

One way to divide learning algorithms is as follows: those whose representation has a fixed size, like linear classifiers, and those whose representation can grow with the data, like decision trees. Fixed-size learners can only take advantage of so much data. Variable-sized learners can in principle learn any function given sufficient data, but in practice may not, because of limitations of the algorithm or computational cost. Also, because of the curse of dimensionality, no existing amount of data may be enough. \cite{Domingos2012} Each type of learner comes with its own assumptions and, in keeping with the 'no free lunch theorum', none are demonstrably best in all situations, although some are clearly better than most. \cite{Hand2006,Delgado2014}

Perhaps because each type of learner is looking at the data from different angles, combining the results of different learners into an ensemble of models is a technique that has demonstrated its effectiveness at improving overall accuracy. Creating model ensembles is now standard in machine learning. The statistics community uses techniques, such as bagging, boosting and stacking to resample training data or reweight classifier inputs on previously misclassified data in order to reduce variance while minimizing bias. But the simplest forms of ensemble of modeling can be thought of as a 'majority vote', where final class assignment of an instance rests with the class arrived at by most of the included models.


Ensembles of models frequently outperform any of their individual, constituent models, so a logical next step of this study would be to combine results from all models developed so far.

