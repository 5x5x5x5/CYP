
%But first I need a simple statement to remind me that I am making progress and that my work is going towards a good end. It makes sense in the context of the program. The more I delved into research the more ambitious my project became. In the course of my graduate career I became a different person in many way, more like I shed the old person in order to survive. I realize that I need a career or job or at least an activity (while basic needs for food shelter and love met elsewhere) that I am grateful for, and I think that means it needs to be challenging and supportive of my growth in the process. I am still a biologist and I think that means I am a student of what works. I think the field of systems biology best represents my view of biology and many fields of inquiry within it are opening due to advances in computing power (size and speed) that enable previously advanced or theoretical mathematics to come to bear on very old problems. Even setting -omics aside, the current state of microscopy and automated image analysis allow for information capture that far exceeds currently utilized analysis methods. I feel like nonparametric/nonlinear/ new machine learning methods are the most fruitful way to proceed. Mathematics is the new microscope. I need to learn tools and techniques and this masters project was a chance to practice and demonstrate them.



\section{Other Attempts at Modeling Assay 1851}
Pubchem Bioassy AID1851 has been the basis for several attempts to advance \textit{in silico} screening development. Several research groups have developed large-scale single target models for the single isoforms. \cite{Vasanthanathan2009, Sridhar2012, Sun2012, Novotarskyi2010} Lapins et al. pursured all five isozymes but used a proteochemometric method that also takes into account each isozyme's protein sequence. \cite{Lapins2013} Cheng et al created single target QSAR models for all five CYP isoforms which are the most comparable to my efforts. \cite{Cheng2011} These models all showed good predictive performances according to our standards of a 0.6 accuracy threshold.

%It is highly desirable to develop computational models that can predict the inhibitive effect of a compound against a specific CYP isoform. 
In Cheng's study inhibitor predicting models were developed for five major CYP isoforms, namely 1A2, 2C9, 2C19, 2D6, and 3A4. He usied a combined classifier algorithm on a large data set containing more than 24,700 unique compounds, that came from PubChem and included the 17,143 used in this study. His group used an ensemble of different independent machine learning classifiers including support vector machine, C 4.5 decision tree, $/kappa$-nearest neighbor, and naive Bayes. The ensemble was brought together by a back-propagation artificial neural network (BP-ANN). Those models were also validated by 5-fold cross-validation  and tested on a test set composed of about 9000 diverse unique compounds previously unseen by the classifier. The range of the AUROC for the validation set was 0.764 to 0.886. The overall performance of combined classifiers fused by BP-ANN was superior to that of three classic fusion techniques (Mean, Maximum, and Multiply). Cheng et al. claim these classification models are applicable for virtual screening of the five major CYP isoforms inhibitors or can be used as simple filters of potential chemicals in drug discovery. \cite{Cheng2011} These results are similar to the findings presented above.


\section{Sources of Error}
This assay required recombinant sources of CYP enzymes, because the probes are not sufficiently P450 isoform-selective to be used with those derived from human liver microsomes. Another potential source for false positives in these assays can be compounds that interfere with light generation directly, such as compounds that interfere with luciferase enzymatic activity. \cite{Zlokarnik2005}

The results of the large-scale screening of Pubchem AID1851 against five CYP isoforms identified that the majority of compounds in a typical chemical library cross-inhibited several isoforms, while only a small fraction of the compounds did not inhibit any of the isoforms. \cite{Veith2009}

The phenomenon of uniformly lower accracy on the inactives versus the actives might be explained by the composition of the inactive class. Both inactive inhibitors as well as compounds of uncertain activity were both included under the label 'inactive' for binary classification purposes. The chosen representation of class inclusion may have less potential for successful discrimination than. To test assumptions about class labels, the number of classes could be increased or the Activity Score threshold could be altered in future models.


\section{Machine Learning for Pharmaceutical Sciences}
\begin{quote}
Developing successful machine learning applications still requires a substantial amount of “black art” that is hard to find in textbooks.\cite{Domingos2012}
\end{quote}

Unlike in this study, raw data is often not in a form that is amenable to learning, but you can construct features from it that are. Easily the most important factor in a good model is the features used. If there are many independent features that correlate well with the class, learning is easy. However if the class is a very complex function of the features, the time required to train a good model may take to long or be impossible. \cite{Domingos2012} Most of the time and effort in a machine learning project goestypically goes into feature engineering, and it is one area where domain expertise and experience with chemical library design can mean the difference between success and failure.

Also machine learning is not a one-shot process of building a data set and running a learner, but rather an iterative process of running the learner, analyzing results, modifying the data and/or learner, and repeating. Learning is often the quickest part of this, but that’s because alot of effort has gone into sharing and codifying that knowledge. Feature engineering is more difficult because it is domain-specific, while learners can be largely general-purpose.\cite{Domingos2012}

There is ultimately no replacement for the smarts you put into feature engineering. On the other hand, running a learner with a very large number of features to find out which ones are useful in combination may be too time-consuming, or cause overfitting.\cite{Domingos2012}

As a rule, it pays to try the simplest learners first(e.g. naive Bayes before logistic regression, $\kappa$-nearest neighbor before support vector machines). More sophisticated learners may be seductive, but they are usually harder to use, because there are more tuning parameters and their internals are more opaque.\cite{Domingos2012}

One way to divide learning algorithms is as follows: those whose representation has a fixed size, like linear classifiers, and those whose representation can grow with the data, like decision trees. Fixed-size learners can only take advantage of so much data. Variable-sized learners can in principle learn any function given sufficient data, but in practice may not, because of limitations of the algorithm or computational cost. Also, because of the curse of dimensionality, no existing amount of data may be enough. \cite{Domingos2012} Each type of learner comes with its own assumptions and, as of yet, none are demonstrably best in all situations, although some are clearly better than most. \cite{Hand2006,Delgado2014}


Perhaps because each type of learner is looking at the data from different angles, combining the results of different learners into an ensemble of models is a technique that has demonstrated its effectiveness at imporving overall accuracy. Creating model ensembles is now standard in machine learning. The statistic community uses techniques, such as bagging, boosting and stacking to resample training data or reweight classifier inputs on previously misclassified data in order to reduce variance while minimizing bias. But in the simplest form of an ensemble of models can be thought of as a 'majority vote', where final class assignment rest at which most of the included models arrived.


Ensembles of models frequently outperform any single model, so a logical next step of this study would be to combine results from all models developed.

